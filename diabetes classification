# Import necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, make_scorer
from sklearn.model_selection import cross_validate

# Load the dataset
df = pd.read_csv('diabetes.csv')

# Basic exploration of the data
print("First 5 rows of the dataset:")
print(df.head())
print("\nDataset description:")
print(df.describe())
print("\nDataset info:")
print(df.info())

# Check for missing values
print("\nChecking for missing values:")
print(df.isnull().values.any())

# Visualizations
print("\nDisplaying histogram of features:")
df.hist(bins=10, figsize=(10, 10))
plt.show()

print("\nDisplaying correlation heatmap:")
sns.heatmap(df.corr(), annot=True, cmap="coolwarm")
plt.show()

print("\nCountplot of outcome:")
sns.countplot(y=df['Outcome'], palette='Set1')
plt.show()

# Pairplot of features colored by outcome
print("\nDisplaying pairplot of features:")
sns.pairplot(df, hue="Outcome")
plt.show()

# Boxplot to visualize outliers
print("\nDisplaying boxplots of features for outlier detection:")
plt.figure(figsize=(15, 6))
df.boxplot()
plt.title("Boxplot for Outlier Visualization")
plt.show()

# Outlier removal using IQR method
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
df_out = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]
print(f"\nOriginal shape: {df.shape}, Shape after outlier removal: {df_out.shape}")

# Scatter matrix after removing outliers
print("\nDisplaying pairplot after outlier removal:")
sns.pairplot(df_out, hue="Outcome")
plt.show()

# Extracting features and target variable
X = df_out.drop(columns=['Outcome'])
y = df_out['Outcome']

# Splitting data into train and test sets
train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"Train shapes: {train_X.shape}, {train_y.shape} | Test shapes: {test_X.shape}, {test_y.shape}")

# Function to display cross-validation results
def display_result(result):
    print(f"TP: {np.mean(result['test_tp'])}, TN: {np.mean(result['test_tn'])}, FN: {np.mean(result['test_fn'])}, FP: {np.mean(result['test_fp'])}")

# Dictionary to store results for plotting
acc = []
roc = []

# List of models to evaluate
models = {
    'Logistic Regression': LogisticRegression(),
    'SVM': SVC(kernel='linear', probability=True),
    'KNN': KNeighborsClassifier(n_neighbors=3),
    'Random Forest': RandomForestClassifier(),
    'Naive Bayes': GaussianNB(),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=50, learning_rate=0.2)
}

# Cross-validation scoring setup
scoring = {
    'accuracy': make_scorer(accuracy_score),
    'tp': make_scorer(lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[1, 1]),
    'tn': make_scorer(lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[0, 0]),
    'fp': make_scorer(lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[0, 1]),
    'fn': make_scorer(lambda y_true, y_pred: confusion_matrix(y_true, y_pred)[1, 0])
}

# Evaluate each model
for model_name, model in models.items():
    print(f"\nEvaluating {model_name}...")
    model.fit(train_X, train_y)
    y_pred = model.predict(test_X)
    
    # Calculate accuracy and ROC AUC
    ac = accuracy_score(test_y, y_pred)
    rc = roc_auc_score(test_y, model.predict_proba(test_X)[:, 1]) if hasattr(model, "predict_proba") else roc_auc_score(test_y, model.decision_function(test_X))
    
    acc.append(ac)
    roc.append(rc)
    
    print(f"Accuracy: {ac:.2f}, ROC AUC: {rc:.2f}")
    
    # Cross-validation
    result = cross_validate(model, train_X, train_y, scoring=scoring, cv=10)
    display_result(result)

# Plotting accuracy and ROC AUC for all models
plt.figure(figsize=(9, 4))
plt.bar(models.keys(), acc, label='Accuracy', color='blue')
plt.ylabel('Accuracy Score')
plt.xlabel('Algorithms')
plt.xticks(rotation=15)
plt.title('Model Accuracy Comparison')
plt.show()

plt.figure(figsize=(9, 4))
plt.bar(models.keys(), roc, label='ROC AUC', color='green')
plt.ylabel('ROC AUC')
plt.xlabel('Algorithms')
plt.xticks(rotation=15)
plt.title('Model ROC AUC Comparison')
plt.show()

