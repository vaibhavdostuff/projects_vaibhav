
import pandas as pd
import requests
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Data Collection from World Bank Climate Portal
def fetch_climate_data():
    # Fetching climate data from the World Bank API (example: temperature anomaly data)
    url = "http://climatedataapi.worldbank.org/climateweb/rest/v1/country/annualavg/tas/2020/2039/IND"  # Example for India
    response = requests.get(url)

    if response.status_code == 200:
        data = response.json()
        df = pd.json_normalize(data, 'data', ['gcm', 'variable', 'model'], errors='ignore')
        return df
    else:
        print("Failed to fetch data. Status Code:", response.status_code)
        return pd.DataFrame()  # Return an empty DataFrame on failure
    
# Load data
data = fetch_climate_data()

# Check if data was fetched successfully
if data.empty:
    print("No data to process.")
else:
    # Step 2: Data Analysis
    # Exploratory Data Analysis (EDA)
    print("Data Overview:")
    print(data.head())
    print("\nSummary Statistics:")
    print(data.describe())

    # Visualize the distribution of climate data
    plt.figure(figsize=(10, 6))
    sns.histplot(data['data'], kde=True)
    plt.title('Distribution of Climate Data')
    plt.xlabel('Temperature Anomaly')
    plt.ylabel('Frequency')
    plt.show()

    # Step 3: Data Processing
    # For simplicity, we'll assume relevant columns are 'year', 'data', 'gcm'
    data = data.dropna()  # Drop missing values

    # Example feature engineering
    features = data[['data']]  # Assume 'data' column represents some climate metric (e.g., temperature anomaly)
    target = data['gcm']       # Assume 'gcm' represents climate classifications (modify as per actual scenario)

     